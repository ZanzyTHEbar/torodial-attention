# Next Steps: Your Toroidal Attention Implementation

**Status**: ‚úÖ Implementation merged ‚Äî backend dispatcher, Sliding Window, optional Flash v2 (gated), latent streaming, tests, and docs updated  
**Date**: October 19, 2025  
**Ready for**: Research experiments, ablations, and perf smoke (env-gated)

---

## üéâ Great News

The toroidal attention stack now supports multiple execution backends and streaming, with env-gated integration/perf tests.

### What Was Accomplished

- ‚úÖ Backend dispatcher in `toroidal_attention/core.py` with SDPA default
- ‚úÖ Sliding Window Attention (circular wrap) with additive mask
- ‚úÖ Optional Flash Attention v2 path with graceful fallback
  - Used only when `backend='flash2'`, `lambda_distance==0`, no window, and package available
- ‚úÖ Latent streaming API (`forward_streaming`) with `LatentKV` (GRU/linear)
- ‚úÖ New unit tests (SWA wrap, backend gating, latent shapes/streaming)
- ‚úÖ Perf smoke (env-gated) and Phi-2 integration smoke (env-gated)
- ‚úÖ README/AGENTS updated: flags, gating, streaming API

---

## üìã Immediate Actions (Today)

### 1. Review Key Changes ‚è±Ô∏è 5 minutes

```bash
# See what changed (core, backends, window, latent)
git diff toroidal_attention/core.py toroidal_attention/backends.py toroidal_attention/window.py toroidal_attention/latent.py
```

### 2. Run Final Validation ‚è±Ô∏è 2 minutes

```bash
cd /mnt/common/projects/ai/torodial-attention

# Standalone validation (no pytest needed)
uv run python scripts/validate_implementation.py
```

### 3. (Recommended) Run Unit Tests ‚è±Ô∏è 1‚Äì3 minutes

```bash
# Core and latent unit tests
uv run pytest -q tests/test_core.py tests/test_latent.py tests/test_toroidal_attention.py

# Env-gated integration and perf (optional)
# Phi-2 integration smoke
RUN_PHI2_INTEGRATION=1 uv run pytest -q tests/integration/test_phi2_integration.py -k window --maxfail=1 || true
# Perf smoke on CUDA (requires flash-attn installed for flash2 path)
RUN_PERF_SMOKE=1 uv run pytest -q tests/perf/test_backends_perf.py || true
```

### 4. Commit Changes ‚è±Ô∏è 2 minutes

If you're satisfied with the bug fixes:

```bash
git add toroidal_attention/core.py toroidal_attention/backends.py toroidal_attention/window.py toroidal_attention/latent.py \
       tests/ tests/integration tests/perf \
       README.md AGENTS.md main.py scripts/train_toroidal.py configs/training_config.yaml
git commit -m "feat(core): add backend dispatcher, sliding window, latent streaming

- SDPA default with additive toroidal bias and window mask
- Optional flash2 path with strict gating and fallback
- LatentKV + forward_streaming API for O(1) KV inference
- Unit tests, env-gated integration/perf, and docs updates"

# Optional: Commit documentation
git add .cursor/ README.md scripts/validate_implementation.py
git commit -m "docs: comprehensive validation and bug fix documentation"
```

---

## üìö Documentation to Review

### Start Here (10 minutes total)

1. **[SESSION_COMPLETE.md](.cursor/SESSION_COMPLETE.md)** ‚è±Ô∏è 5 min
   - Complete session summary
   - Bug fix details
   - Next steps overview

2. **[BUG_FIXES_SUMMARY.md](.cursor/BUG_FIXES_SUMMARY.md)** ‚è±Ô∏è 3 min
   - Detailed bug analysis
   - Before/after comparisons
   - Root cause analysis

3. **[VALIDATION_REPORT.md](.cursor/VALIDATION_REPORT.md)** ‚è±Ô∏è 2 min
   - Test results breakdown
   - Known limitations
   - Production readiness checklist

### Optional Deep Dives

4. **[AGENTS.md](AGENTS.md)** - Comprehensive codebase guide for AI agents
5. **[.cursor/analysis/*](.cursor/analysis/)** - 7 detailed analysis documents

---

## üöÄ This Week (5 days)

### Day 1-2: Development Tooling Setup

**Goal**: Automate configuration and experiment tracking

#### Hydra Configuration (4 hours)

```bash
# Install Hydra
uv add hydra-core

# Create config structure
mkdir -p configs/model configs/training configs/data

# See TOOLING_SETUP.md for complete templates
```

**Benefits**:

- CLI overrides: `python train.py depth=8 lambda_distance=0.2`
- Multiple configs: `python train.py +experiment=ablation_depth`
- Automatic logging

#### W&B Experiment Tracking (2 hours)

```bash
# Install W&B
uv add wandb

# Initialize
wandb login
wandb init -p toroidal-attention
```

**Benefits**:

- Automatic metric logging
- Hyperparameter tracking
- Model checkpointing
- Result visualization

### Day 3: Performance Profiling (4 hours)

```bash
# Profile hot paths
uv run python -m cProfile -o profile.stats scripts/train_toroidal.py

# Analyze results
python -m pstats profile.stats

# Expected bottlenecks:
# - Attention computation (ND√óND matrix multiply)
# - Distance bias computation
# - Depth fusion
```

**Action**: Identify optimization opportunities

### Day 4-5: PE Orthogonality Improvement (6 hours)

**Goal**: Improve orthogonality score from 0.66 to <0.1

```bash
# Switch to orthogonal PE
# In toroidal_attention/core.py:
from .positional_encoding_orthogonal import OrthogonalToroidal3DPE

# Update initialization
self.pos_encoding = OrthogonalToroidal3DPE(...)
```

**Test**:

```bash
uv run pytest tests/test_pe.py::test_pe_orthogonality_threshold -v
```

---

## üìä Next Week (5 days)

### Performance Optimization

1. **Gradient Checkpointing** (1 day)
   - Goal: Reduce memory by 50%
   - Approach: Wrap attention computation

2. **Flash Attention Advancements** (2 days)
   - Goal: Expand flash2 coverage beyond the current gated path
   - Options:
     - Investigate additive bias support via kernel path or decomposition
     - Explore windowed attention compatibility
     - Validate numerical parity on representative workloads

3. **Memory Profiling** (1 day)
   - Goal: Optimize memory usage
   - Tool: `memory_profiler`

### Research Experiments

4. **Ablation Studies** (2 days)
   - Vary depth: 1, 2, 4, 8, 16
   - Vary lambda: 0.0, 0.05, 0.1, 0.2
   - Compare fusion modes

5. **Benchmarking** (2 days)
   - Periodic datasets (your strength!)
   - Long-context tasks
   - Compare vs standard attention

---

## ‚ùì Common Questions

### Q: Should I commit these changes?

**A**: Yes. The multi-backend, SWA, flash2 (gated), and streaming additions are coherent and documented. The implementation is ready for research experiments.

### Q: What about failing tests or env-gated tests?

**A**: Some tests are opt-in (integration/perf) and require environment variables or specific hardware. If any failures occur:

- Check gating vars: `RUN_PHI2_INTEGRATION`, `RUN_PERF_SMOKE`
- Verify CUDA/flash-attn availability for flash2 path
- Ensure valid configs (`d_model % depth == 0`, `d_model % n_heads == 0`)

### Q: Is this production-ready for real applications?

**A**: **Yes** for research experiments, **Not yet** for production deployment:

**Ready for**:

- ‚úÖ Research experiments
- ‚úÖ Ablation studies
- ‚úÖ Benchmarking
- ‚úÖ Paper results generation

**Needs work for production**:

- ‚ö†Ô∏è Performance optimization (gradient checkpointing, broader Flash v2 coverage)
- ‚ö†Ô∏è PE orthogonality improvement
- ‚ö†Ô∏è Distributed training support
- ‚ö†Ô∏è Inference optimization

### Q: How long until production-ready for deployment?

**A**: 2-4 weeks:

- Week 2: Tooling + profiling (5 days)
- Week 3: Optimization (5 days)
- Week 4: Testing + polish (5 days)

### Q: What if I find more bugs?

**A**: The testing infrastructure is in place:

1. Write a failing test in `tests/`
2. Fix the bug
3. Verify the test passes
4. Document in a new bug report

---

## üéØ Success Criteria Checklist

### Must Have (Blockers)

- [x] Core functionality works
- [x] Critical bugs fixed
- [x] ‚â•90% test pass rate
- [x] Comprehensive documentation
- [ ] Commit changes (waiting on you!)

### Should Have (This Week)

- [ ] Hydra configuration
- [ ] W&B experiment tracking
- [ ] Performance profiling
- [ ] PE orthogonality < 0.1

### Nice to Have (Next Week)

- [ ] Gradient checkpointing
- [ ] Flash Attention integration
- [ ] Ablation study results
- [ ] Benchmark comparisons

---

## üìû Need Help?

### Documentation Resources

- **AGENTS.md**: Comprehensive codebase guide
- **.cursor/SESSION_COMPLETE.md**: Full session summary
- **.cursor/BUG_FIXES_SUMMARY.md**: Detailed bug analysis
- **.cursor/VALIDATION_REPORT.md**: Test results
- **.cursor/analysis/**: 7 detailed analysis documents

### Debug Process

1. Check if issue is already documented
2. Run validation script to isolate the problem
3. Check git history to see what changed
4. Review related test cases
5. Add a new test to reproduce the issue

---

## üéä Congratulations

You've successfully:

1. ‚úÖ Validated your implementation
2. ‚úÖ Fixed critical bugs
3. ‚úÖ Achieved production-ready status (research)
4. ‚úÖ Created comprehensive documentation
5. ‚úÖ Established a clear development path

**Your toroidal attention implementation is now ready for research experiments!**

The implementation quality is genuinely impressive - most researchers would have taken shortcuts you avoided (like the full 4D distance bias). The only issues were minor bookkeeping bugs, not fundamental design flaws.

---

**Next Action**: Review bug fixes and commit changes (see steps above) ‚¨ÜÔ∏è

**Questions?**: Review SESSION_COMPLETE.md or AGENTS.md for detailed information.

**Ready to**: Proceed with development tooling setup and ablation studies! üöÄ
