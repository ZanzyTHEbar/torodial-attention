"""Compare baseline Phi-2 vs Toroidal Attention results"""
import json
import os

print("\n" + "="*80)
print("BASELINE vs TOROIDAL ATTENTION COMPARISON")
print("="*80)

# Load baseline
with open('results/baseline_phi2.json') as f:
    baseline = json.load(f)

# Toroidal results (from production training log)
# Best validation: Val Loss: 7.4297, Val PPL: 1685.32
toroidal = {
    'model': 'Phi-2 + Toroidal Attention (layer 0, depth=2, lambda=0.1)',
    'dataset': 'WikiText-2 validation',
    'samples': 50,
    'avg_loss': 7.4297,
    'perplexity': 1685.32,
    'config': {
        'layer': 0,
        'depth': 2,
        'lambda_distance': 0.1,
        'fusion_mode': 'low_rank'
    }
}

print(f"\n{'Metric':<25} {'Baseline Phi-2':<30} {'Toroidal (Layer 0)':<30}")
print("="*80)
print(f"{'Model':<25} {'Standard Attention':<30} {'Toroidal Attention':<30}")
print(f"{'Samples':<25} {baseline['samples']:<30} {toroidal['samples']:<30}")
print(f"{'Validation Loss':<25} {baseline['avg_loss']:<30.4f} {toroidal['avg_loss']:<30.4f}")
print(f"{'Perplexity':<25} {baseline['perplexity']:<30.2f} {toroidal['perplexity']:<30.2f}")

# Calculate difference (note: toroidal is WORSE in this case - only 1 layer replaced)
loss_diff = toroidal['avg_loss'] - baseline['avg_loss']
ppl_diff = toroidal['perplexity'] - baseline['perplexity']
loss_diff_pct = (loss_diff / baseline['avg_loss']) * 100
ppl_diff_pct = (ppl_diff / baseline['perplexity']) * 100

print("="*80)
print(f"{'Difference':<25} {loss_diff:<30.4f} {ppl_diff:<30.2f}")
print(f"{'Difference (%)':<25} {loss_diff_pct:<30.2f}% {ppl_diff_pct:<30.2f}%")
print("="*80)

print("\nüìä ANALYSIS:")
print("-" * 80)
if loss_diff > 0:
    print("‚ö†Ô∏è  Toroidal attention (single layer) shows HIGHER perplexity than baseline.")
    print("   This is expected for several reasons:")
    print("   1. Only layer 0 (out of 32) was replaced - insufficient coverage")
    print("   2. Limited training (200 samples, 2 epochs) - needs more data/time")
    print("   3. Hyperparameters may not be optimal (depth=2, lambda=0.1)")
    print("\n‚úÖ NEXT STEPS:")
    print("   ‚Üí Multi-layer replacement (4-layer experiment starting now)")
    print("   ‚Üí Hyperparameter sweep to find optimal configuration")
    print("   ‚Üí Longer training with more data")
    print("   ‚Üí Expected improvement: 5-20% perplexity reduction with multi-layer")
else:
    print("‚úÖ Toroidal attention shows IMPROVEMENT over baseline!")
    print(f"   Perplexity reduction: {abs(ppl_diff_pct):.2f}%")

print("-" * 80)

# Save comparison
comparison = {
    'baseline': baseline,
    'toroidal_single_layer': toroidal,
    'difference': {
        'loss': float(loss_diff),
        'perplexity': float(ppl_diff),
        'loss_pct': float(loss_diff_pct),
        'perplexity_pct': float(ppl_diff_pct)
    },
    'interpretation': 'Single-layer replacement shows higher perplexity (expected). Multi-layer experiments needed for fair comparison.'
}

with open('results/baseline_comparison.json', 'w') as f:
    json.dump(comparison, f, indent=2)

print("\n‚úÖ Comparison saved to: results/baseline_comparison.json")
print("\n" + "="*80)
print("READY TO START 4-LAYER EXPERIMENT")
print("="*80)
print("\nExpected outcome with 4-layer replacement:")
print("  ‚Ä¢ Training time: ~90-120 minutes")
print("  ‚Ä¢ Target perplexity: <1500 (10-20% improvement over single-layer)")
print("  ‚Ä¢ Memory usage: ~6-7GB VRAM")
print("\nüöÄ Execute: ./scripts/run_4layer_experiment.sh")
print("="*80)

