`torch_dtype` is deprecated! Use `dtype` instead!
================================================================================
FP32 OPTIMIZER TEST
================================================================================

[1/3] Loading model (fp16)...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 27.86it/s]

Replacing attention in layer 0 with ToroidalAttention...
  Copying weights from original attention...
  ✓ Weights copied successfully
  ✓ Replaced model.layers[0].self_attn
  ToroidalAttention parameters: 26,224,644

Freezing model parameters...
  Total parameters: 2,779,683,844
  Trainable parameters: 26,224,644 (0.94%)
  Frozen parameters: 2,753,459,200

[2/3] Creating FP32 optimizer (master weights)...
   Model params dtype: torch.float16
   Master params dtype: torch.float32
   Total trainable params: 26,224,644

[3/3] Running 10 training steps with FP32 master weights...
   Step  1: Loss= 0.2465, MaxGrad=1.59e+01 → 1.45e-01 ✅
   Step  2: Loss= 0.2513, MaxGrad=2.12e+01 → 1.67e-01 ✅
   Step  3: Loss= 0.2915, MaxGrad=1.42e+01 → 1.25e-01 ✅
   Step  4: Loss= 0.3501, MaxGrad=3.10e+01 → 1.76e-01 ✅
   Step  5: Loss= 0.5284, MaxGrad=4.26e+01 → 2.05e-01 ✅
   Step  6: Loss= 0.5976, MaxGrad=4.30e+01 → 1.42e-01 ✅
   Step  7: Loss= 0.5401, MaxGrad=4.13e+01 → 1.46e-01 ✅
   Step  8: Loss= 0.5267, MaxGrad=9.02e+01 → 1.47e-01 ✅
   Step  9: Loss= 0.4515, MaxGrad=8.99e+01 → 1.48e-01 ✅
   Step 10: Loss= 0.5761, MaxGrad=4.72e+01 → 1.12e-01 ✅

================================================================================
RESULT: FP32 master weights approach
================================================================================
[W1025 22:24:09.499397419 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
